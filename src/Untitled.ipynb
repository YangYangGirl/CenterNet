{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _init_paths\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import argparse\n",
    "from detectors.detector_factory import detector_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ext = ['jpg', 'jpeg', 'png', 'webp']\n",
    "video_ext = ['mp4', 'mov', 'avi', 'mkv']\n",
    "time_stats = ['tot', 'load', 'pre', 'net', 'dec', 'post', 'merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parser.add_argument(\\'--dataset\\', default=\\'coco\\',\\n                             help=\\'coco | kitti | coco_hp | pascal\\')\\nparser.add_argument(\\'--exp_id\\', default=\\'default\\')\\nparser.add_argument(\\'--test\\', action=\\'store_true\\')\\nparser.add_argument(\\'--debug\\', type=int, default=0,\\n                             help=\\'level of visualization.\\'\\n                                  \\'1: only show the final detection results\\'\\n                                  \\'2: show the network output features\\'\\n                                  \\'3: use matplot to display\\' # useful when lunching training with ipython notebook\\n                                  \\'4: save all visualizations to disk\\')\\nparser.add_argument(\\'--demo\\', default=\\'\\', \\n                             help=\\'path to image/ image folders/ video. \\'\\n                                  \\'or \"webcam\"\\')\\nparser.add_argument(\\'--load_model\\', default=\\'\\',\\n                             help=\\'path to pretrained model\\')\\nparser.add_argument(\\'--resume\\', action=\\'store_true\\',\\n                             help=\\'resume an experiment. \\'\\n                                  \\'Reloaded the optimizer parameter and \\'\\n                                  \\'set load_model to model_last.pth \\'\\n                                  \\'in the exp dir if load_model is empty.\\') \\n\\n# system\\nparser.add_argument(\\'--gpus\\', default=\\'0\\', \\n                             help=\\'-1 for CPU, use comma for multiple gpus\\')\\nparser.add_argument(\\'--num_workers\\', type=int, default=4,\\n                             help=\\'dataloader threads. 0 for single-thread.\\')\\nparser.add_argument(\\'--not_cuda_benchmark\\', action=\\'store_true\\',\\n                             help=\\'disable when the input size is not fixed.\\')\\nparser.add_argument(\\'--seed\\', type=int, default=317, \\n                             help=\\'random seed\\') # from CornerNet\\n\\n# log\\nparser.add_argument(\\'--print_iter\\', type=int, default=0, \\n                             help=\\'disable progress bar and print to screen.\\')\\nparser.add_argument(\\'--hide_data_time\\', action=\\'store_true\\',\\n                             help=\\'not display time during training.\\')\\nparser.add_argument(\\'--save_all\\', action=\\'store_true\\',\\n                             help=\\'save model to disk every 5 epochs.\\')\\nparser.add_argument(\\'--metric\\', default=\\'loss\\', \\n                             help=\\'main metric to save best model\\')\\nparser.add_argument(\\'--vis_thresh\\', type=float, default=0.3,\\n                             help=\\'visualization threshold.\\')\\nparser.add_argument(\\'--debugger_theme\\', default=\\'white\\', \\n                             choices=[\\'white\\', \\'black\\'])\\n    \\n# model\\nparser.add_argument(\\'--arch\\', default=\\'dla_34\\', \\n                             help=\\'model architecture. Currently tested\\'\\n                                  \\'res_18 | res_101 | resdcn_18 | resdcn_101 |\\'\\n                                  \\'dlav0_34 | dla_34 | hourglass\\')\\nparser.add_argument(\\'--head_conv\\', type=int, default=-1,\\n                             help=\\'conv layer channels for output head\\'\\n                                  \\'0 for no conv layer\\'\\n                                  \\'-1 for default setting: \\'\\n                                  \\'64 for resnets and 256 for dla.\\')\\nparser.add_argument(\\'--down_ratio\\', type=int, default=4,\\n                             help=\\'output stride. Currently only supports 4.\\')\\n\\n# input\\nparser.add_argument(\\'--input_res\\', type=int, default=-1, \\n                             help=\\'input height and width. -1 for default from \\'\\n                             \\'dataset. Will be overriden by input_h | input_w\\')\\nparser.add_argument(\\'--input_h\\', type=int, default=-1, \\n                             help=\\'input height. -1 for default from dataset.\\')\\nparser.add_argument(\\'--input_w\\', type=int, default=-1, \\n                             help=\\'input width. -1 for default from dataset.\\')\\n    \\n# train\\nparser.add_argument(\\'--lr\\', type=float, default=1.25e-4, \\n                             help=\\'learning rate for batch size 32.\\')\\nparser.add_argument(\\'--lr_step\\', type=str, default=\\'90,120\\',\\n                             help=\\'drop learning rate by 10.\\')\\nparser.add_argument(\\'--num_epochs\\', type=int, default=140,\\n                             help=\\'total training epochs.\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=32,\\n                             help=\\'batch size\\')\\nparser.add_argument(\\'--master_batch_size\\', type=int, default=-1,\\n                             help=\\'batch size on the master gpu.\\')\\nparser.add_argument(\\'--num_iters\\', type=int, default=-1,\\n                             help=\\'default: #samples / batch_size.\\')\\nparser.add_argument(\\'--val_intervals\\', type=int, default=5,\\n                             help=\\'number of epochs to run validation.\\')\\nparser.add_argument(\\'--trainval\\', action=\\'store_true\\',\\n                             help=\\'include validation in training and \\'\\n                                  \\'test on test set\\')\\n\\n# test\\nparser.add_argument(\\'--flip_test\\', action=\\'store_true\\',\\n                             help=\\'flip data augmentation.\\')\\nparser.add_argument(\\'--test_scales\\', type=str, default=\\'1\\',\\n                             help=\\'multi scale test augmentation.\\')\\nparser.add_argument(\\'--nms\\', action=\\'store_true\\',\\n                             help=\\'run nms in testing.\\')\\nparser.add_argument(\\'--K\\', type=int, default=100,\\n                             help=\\'max number of output objects.\\') \\nparser.add_argument(\\'--not_prefetch_test\\', action=\\'store_true\\',\\n                             help=\\'not use parallal data pre-processing.\\')\\nparser.add_argument(\\'--fix_res\\', action=\\'store_true\\',\\n                             help=\\'fix testing resolution or keep \\'\\n                                  \\'the original resolution\\')\\nparser.add_argument(\\'--keep_res\\', action=\\'store_true\\',\\n                             help=\\'keep the original resolution\\'\\n                                  \\' during validation.\\')\\n\\n# dataset\\nparser.add_argument(\\'--not_rand_crop\\', action=\\'store_true\\',\\n                             help=\\'not use the random crop data augmentation\\'\\n                                  \\'from CornerNet.\\')\\nparser.add_argument(\\'--shift\\', type=float, default=0.1,\\n                             help=\\'when not using random crop\\'\\n                                  \\'apply shift augmentation.\\')\\nparser.add_argument(\\'--scale\\', type=float, default=0.4,\\n                             help=\\'when not using random crop\\'\\n                                  \\'apply scale augmentation.\\')\\nparser.add_argument(\\'--rotate\\', type=float, default=0,\\n                             help=\\'when not using random crop\\'\\n                                  \\'apply rotation augmentation.\\')\\nparser.add_argument(\\'--flip\\', type = float, default=0.5,\\n                             help=\\'probability of applying flip augmentation.\\')\\nparser.add_argument(\\'--no_color_aug\\', action=\\'store_true\\',\\n                             help=\\'not use the color augmenation \\'\\n                                  \\'from CornerNet\\')\\n# multi_pose\\nparser.add_argument(\\'--aug_rot\\', type=float, default=0, \\n                             help=\\'probability of applying \\'\\n                                  \\'rotation augmentation.\\')\\n# ddd\\nparser.add_argument(\\'--aug_ddd\\', type=float, default=0.5,\\n                             help=\\'probability of applying crop augmentation.\\')\\nparser.add_argument(\\'--rect_mask\\', action=\\'store_true\\',\\n                             help=\\'for ignored object, apply mask on the \\'\\n                                  \\'rectangular region or just center point.\\')\\nparser.add_argument(\\'--kitti_split\\', default=\\'3dop\\',\\n                             help=\\'different validation split for kitti: \\'\\n                                  \\'3dop | subcnn\\')\\n\\n# loss\\nparser.add_argument(\\'--mse_loss\\', action=\\'store_true\\',\\n                             help=\\'use mse loss or focal loss to train \\'\\n                                  \\'keypoint heatmaps.\\')\\n# ctdet\\nparser.add_argument(\\'--reg_loss\\', default=\\'l1\\',\\n                             help=\\'regression loss: sl1 | l1 | l2\\')\\nparser.add_argument(\\'--hm_weight\\', type=float, default=1,\\n                             help=\\'loss weight for keypoint heatmaps.\\')\\nparser.add_argument(\\'--off_weight\\', type=float, default=1,\\n                             help=\\'loss weight for keypoint local offsets.\\')\\nparser.add_argument(\\'--wh_weight\\', type=float, default=0.1,\\n                             help=\\'loss weight for bounding box size.\\')\\n# multi_pose\\nparser.add_argument(\\'--hp_weight\\', type=float, default=1,\\n                             help=\\'loss weight for human pose offset.\\')\\nparser.add_argument(\\'--hm_hp_weight\\', type=float, default=1,\\n                             help=\\'loss weight for human keypoint heatmap.\\')\\n# ddd\\nparser.add_argument(\\'--dep_weight\\', type=float, default=1,\\n                             help=\\'loss weight for depth.\\')\\nparser.add_argument(\\'--dim_weight\\', type=float, default=1,\\n                             help=\\'loss weight for 3d bounding box size.\\')\\nparser.add_argument(\\'--rot_weight\\', type=float, default=1,\\n                             help=\\'loss weight for orientation.\\')\\nparser.add_argument(\\'--peak_thresh\\', type=float, default=0.2)\\n    \\n# task\\n# ctdet\\nparser.add_argument(\\'--norm_wh\\', action=\\'store_true\\',\\n                             help=\\'L1(\\\\hat(y) / y, 1) or L1(\\\\hat(y), y)\\')\\nparser.add_argument(\\'--dense_wh\\', action=\\'store_true\\',\\n                             help=\\'apply weighted regression near center or \\'\\n                                  \\'just apply regression on center point.\\')\\nparser.add_argument(\\'--cat_spec_wh\\', action=\\'store_true\\',\\n                             help=\\'category specific bounding box size.\\')\\nparser.add_argument(\\'--not_reg_offset\\', action=\\'store_true\\',\\n                             help=\\'not regress local offset.\\')\\n# exdet\\nparser.add_argument(\\'--agnostic_ex\\', action=\\'store_true\\',\\n                             help=\\'use category agnostic extreme points.\\')\\nparser.add_argument(\\'--scores_thresh\\', type=float, default=0.1,\\n                             help=\\'threshold for extreme point heatmap.\\')\\nparser.add_argument(\\'--center_thresh\\', type=float, default=0.1,\\n                             help=\\'threshold for centermap.\\')\\nparser.add_argument(\\'--aggr_weight\\', type=float, default=0.0,\\n                             help=\\'edge aggregation weight.\\')\\n# multi_pose\\nparser.add_argument(\\'--dense_hp\\', action=\\'store_true\\',\\n                             help=\\'apply weighted pose regression near center \\'\\n                                  \\'or just apply regression on center point.\\')\\nparser.add_argument(\\'--not_hm_hp\\', action=\\'store_true\\',\\n                             help=\\'not estimate human joint heatmap, \\'\\n                                  \\'directly use the joint offset from center.\\')\\nparser.add_argument(\\'--not_reg_hp_offset\\', action=\\'store_true\\',\\n                             help=\\'not regress local offset for \\'\\n                                  \\'human joint heatmaps.\\')\\nparser.add_argument(\\'--not_reg_bbox\\', action=\\'store_true\\',\\n                             help=\\'not regression bounding box size.\\')\\n    \\n# ground truth validation\\nparser.add_argument(\\'--eval_oracle_hm\\', action=\\'store_true\\', \\n                             help=\\'use ground center heatmap.\\')\\nparser.add_argument(\\'--eval_oracle_wh\\', action=\\'store_true\\', \\n                             help=\\'use ground truth bounding box size.\\')\\nparser.add_argument(\\'--eval_oracle_offset\\', action=\\'store_true\\', \\n                             help=\\'use ground truth local heatmap offset.\\')\\nparser.add_argument(\\'--eval_oracle_kps\\', action=\\'store_true\\', \\n                             help=\\'use ground truth human pose offset.\\')\\nparser.add_argument(\\'--eval_oracle_hmhp\\', action=\\'store_true\\', \\n                             help=\\'use ground truth human joint heatmaps.\\')\\nparser.add_argument(\\'--eval_oracle_hp_offset\\', action=\\'store_true\\', \\n                             help=\\'use ground truth human joint local offset.\\')\\nparser.add_argument(\\'--eval_oracle_dep\\', action=\\'store_true\\', \\n                             help=\\'use ground truth depth.\\') '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# basic experiment setting\n",
    "parser.add_argument('task', default='ctdet',\n",
    "                             help='ctdet | ddd | multi_pose | exdet')\n",
    "'''parser.add_argument('--dataset', default='coco',\n",
    "                             help='coco | kitti | coco_hp | pascal')\n",
    "parser.add_argument('--exp_id', default='default')\n",
    "parser.add_argument('--test', action='store_true')\n",
    "parser.add_argument('--debug', type=int, default=0,\n",
    "                             help='level of visualization.'\n",
    "                                  '1: only show the final detection results'\n",
    "                                  '2: show the network output features'\n",
    "                                  '3: use matplot to display' # useful when lunching training with ipython notebook\n",
    "                                  '4: save all visualizations to disk')\n",
    "parser.add_argument('--demo', default='', \n",
    "                             help='path to image/ image folders/ video. '\n",
    "                                  'or \"webcam\"')\n",
    "parser.add_argument('--load_model', default='',\n",
    "                             help='path to pretrained model')\n",
    "parser.add_argument('--resume', action='store_true',\n",
    "                             help='resume an experiment. '\n",
    "                                  'Reloaded the optimizer parameter and '\n",
    "                                  'set load_model to model_last.pth '\n",
    "                                  'in the exp dir if load_model is empty.') \n",
    "\n",
    "# system\n",
    "parser.add_argument('--gpus', default='0', \n",
    "                             help='-1 for CPU, use comma for multiple gpus')\n",
    "parser.add_argument('--num_workers', type=int, default=4,\n",
    "                             help='dataloader threads. 0 for single-thread.')\n",
    "parser.add_argument('--not_cuda_benchmark', action='store_true',\n",
    "                             help='disable when the input size is not fixed.')\n",
    "parser.add_argument('--seed', type=int, default=317, \n",
    "                             help='random seed') # from CornerNet\n",
    "\n",
    "# log\n",
    "parser.add_argument('--print_iter', type=int, default=0, \n",
    "                             help='disable progress bar and print to screen.')\n",
    "parser.add_argument('--hide_data_time', action='store_true',\n",
    "                             help='not display time during training.')\n",
    "parser.add_argument('--save_all', action='store_true',\n",
    "                             help='save model to disk every 5 epochs.')\n",
    "parser.add_argument('--metric', default='loss', \n",
    "                             help='main metric to save best model')\n",
    "parser.add_argument('--vis_thresh', type=float, default=0.3,\n",
    "                             help='visualization threshold.')\n",
    "parser.add_argument('--debugger_theme', default='white', \n",
    "                             choices=['white', 'black'])\n",
    "    \n",
    "# model\n",
    "parser.add_argument('--arch', default='dla_34', \n",
    "                             help='model architecture. Currently tested'\n",
    "                                  'res_18 | res_101 | resdcn_18 | resdcn_101 |'\n",
    "                                  'dlav0_34 | dla_34 | hourglass')\n",
    "parser.add_argument('--head_conv', type=int, default=-1,\n",
    "                             help='conv layer channels for output head'\n",
    "                                  '0 for no conv layer'\n",
    "                                  '-1 for default setting: '\n",
    "                                  '64 for resnets and 256 for dla.')\n",
    "parser.add_argument('--down_ratio', type=int, default=4,\n",
    "                             help='output stride. Currently only supports 4.')\n",
    "\n",
    "# input\n",
    "parser.add_argument('--input_res', type=int, default=-1, \n",
    "                             help='input height and width. -1 for default from '\n",
    "                             'dataset. Will be overriden by input_h | input_w')\n",
    "parser.add_argument('--input_h', type=int, default=-1, \n",
    "                             help='input height. -1 for default from dataset.')\n",
    "parser.add_argument('--input_w', type=int, default=-1, \n",
    "                             help='input width. -1 for default from dataset.')\n",
    "    \n",
    "# train\n",
    "parser.add_argument('--lr', type=float, default=1.25e-4, \n",
    "                             help='learning rate for batch size 32.')\n",
    "parser.add_argument('--lr_step', type=str, default='90,120',\n",
    "                             help='drop learning rate by 10.')\n",
    "parser.add_argument('--num_epochs', type=int, default=140,\n",
    "                             help='total training epochs.')\n",
    "parser.add_argument('--batch_size', type=int, default=32,\n",
    "                             help='batch size')\n",
    "parser.add_argument('--master_batch_size', type=int, default=-1,\n",
    "                             help='batch size on the master gpu.')\n",
    "parser.add_argument('--num_iters', type=int, default=-1,\n",
    "                             help='default: #samples / batch_size.')\n",
    "parser.add_argument('--val_intervals', type=int, default=5,\n",
    "                             help='number of epochs to run validation.')\n",
    "parser.add_argument('--trainval', action='store_true',\n",
    "                             help='include validation in training and '\n",
    "                                  'test on test set')\n",
    "\n",
    "# test\n",
    "parser.add_argument('--flip_test', action='store_true',\n",
    "                             help='flip data augmentation.')\n",
    "parser.add_argument('--test_scales', type=str, default='1',\n",
    "                             help='multi scale test augmentation.')\n",
    "parser.add_argument('--nms', action='store_true',\n",
    "                             help='run nms in testing.')\n",
    "parser.add_argument('--K', type=int, default=100,\n",
    "                             help='max number of output objects.') \n",
    "parser.add_argument('--not_prefetch_test', action='store_true',\n",
    "                             help='not use parallal data pre-processing.')\n",
    "parser.add_argument('--fix_res', action='store_true',\n",
    "                             help='fix testing resolution or keep '\n",
    "                                  'the original resolution')\n",
    "parser.add_argument('--keep_res', action='store_true',\n",
    "                             help='keep the original resolution'\n",
    "                                  ' during validation.')\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--not_rand_crop', action='store_true',\n",
    "                             help='not use the random crop data augmentation'\n",
    "                                  'from CornerNet.')\n",
    "parser.add_argument('--shift', type=float, default=0.1,\n",
    "                             help='when not using random crop'\n",
    "                                  'apply shift augmentation.')\n",
    "parser.add_argument('--scale', type=float, default=0.4,\n",
    "                             help='when not using random crop'\n",
    "                                  'apply scale augmentation.')\n",
    "parser.add_argument('--rotate', type=float, default=0,\n",
    "                             help='when not using random crop'\n",
    "                                  'apply rotation augmentation.')\n",
    "parser.add_argument('--flip', type = float, default=0.5,\n",
    "                             help='probability of applying flip augmentation.')\n",
    "parser.add_argument('--no_color_aug', action='store_true',\n",
    "                             help='not use the color augmenation '\n",
    "                                  'from CornerNet')\n",
    "# multi_pose\n",
    "parser.add_argument('--aug_rot', type=float, default=0, \n",
    "                             help='probability of applying '\n",
    "                                  'rotation augmentation.')\n",
    "# ddd\n",
    "parser.add_argument('--aug_ddd', type=float, default=0.5,\n",
    "                             help='probability of applying crop augmentation.')\n",
    "parser.add_argument('--rect_mask', action='store_true',\n",
    "                             help='for ignored object, apply mask on the '\n",
    "                                  'rectangular region or just center point.')\n",
    "parser.add_argument('--kitti_split', default='3dop',\n",
    "                             help='different validation split for kitti: '\n",
    "                                  '3dop | subcnn')\n",
    "\n",
    "# loss\n",
    "parser.add_argument('--mse_loss', action='store_true',\n",
    "                             help='use mse loss or focal loss to train '\n",
    "                                  'keypoint heatmaps.')\n",
    "# ctdet\n",
    "parser.add_argument('--reg_loss', default='l1',\n",
    "                             help='regression loss: sl1 | l1 | l2')\n",
    "parser.add_argument('--hm_weight', type=float, default=1,\n",
    "                             help='loss weight for keypoint heatmaps.')\n",
    "parser.add_argument('--off_weight', type=float, default=1,\n",
    "                             help='loss weight for keypoint local offsets.')\n",
    "parser.add_argument('--wh_weight', type=float, default=0.1,\n",
    "                             help='loss weight for bounding box size.')\n",
    "# multi_pose\n",
    "parser.add_argument('--hp_weight', type=float, default=1,\n",
    "                             help='loss weight for human pose offset.')\n",
    "parser.add_argument('--hm_hp_weight', type=float, default=1,\n",
    "                             help='loss weight for human keypoint heatmap.')\n",
    "# ddd\n",
    "parser.add_argument('--dep_weight', type=float, default=1,\n",
    "                             help='loss weight for depth.')\n",
    "parser.add_argument('--dim_weight', type=float, default=1,\n",
    "                             help='loss weight for 3d bounding box size.')\n",
    "parser.add_argument('--rot_weight', type=float, default=1,\n",
    "                             help='loss weight for orientation.')\n",
    "parser.add_argument('--peak_thresh', type=float, default=0.2)\n",
    "    \n",
    "# task\n",
    "# ctdet\n",
    "parser.add_argument('--norm_wh', action='store_true',\n",
    "                             help='L1(\\hat(y) / y, 1) or L1(\\hat(y), y)')\n",
    "parser.add_argument('--dense_wh', action='store_true',\n",
    "                             help='apply weighted regression near center or '\n",
    "                                  'just apply regression on center point.')\n",
    "parser.add_argument('--cat_spec_wh', action='store_true',\n",
    "                             help='category specific bounding box size.')\n",
    "parser.add_argument('--not_reg_offset', action='store_true',\n",
    "                             help='not regress local offset.')\n",
    "# exdet\n",
    "parser.add_argument('--agnostic_ex', action='store_true',\n",
    "                             help='use category agnostic extreme points.')\n",
    "parser.add_argument('--scores_thresh', type=float, default=0.1,\n",
    "                             help='threshold for extreme point heatmap.')\n",
    "parser.add_argument('--center_thresh', type=float, default=0.1,\n",
    "                             help='threshold for centermap.')\n",
    "parser.add_argument('--aggr_weight', type=float, default=0.0,\n",
    "                             help='edge aggregation weight.')\n",
    "# multi_pose\n",
    "parser.add_argument('--dense_hp', action='store_true',\n",
    "                             help='apply weighted pose regression near center '\n",
    "                                  'or just apply regression on center point.')\n",
    "parser.add_argument('--not_hm_hp', action='store_true',\n",
    "                             help='not estimate human joint heatmap, '\n",
    "                                  'directly use the joint offset from center.')\n",
    "parser.add_argument('--not_reg_hp_offset', action='store_true',\n",
    "                             help='not regress local offset for '\n",
    "                                  'human joint heatmaps.')\n",
    "parser.add_argument('--not_reg_bbox', action='store_true',\n",
    "                             help='not regression bounding box size.')\n",
    "    \n",
    "# ground truth validation\n",
    "parser.add_argument('--eval_oracle_hm', action='store_true', \n",
    "                             help='use ground center heatmap.')\n",
    "parser.add_argument('--eval_oracle_wh', action='store_true', \n",
    "                             help='use ground truth bounding box size.')\n",
    "parser.add_argument('--eval_oracle_offset', action='store_true', \n",
    "                             help='use ground truth local heatmap offset.')\n",
    "parser.add_argument('--eval_oracle_kps', action='store_true', \n",
    "                             help='use ground truth human pose offset.')\n",
    "parser.add_argument('--eval_oracle_hmhp', action='store_true', \n",
    "                             help='use ground truth human joint heatmaps.')\n",
    "parser.add_argument('--eval_oracle_hp_offset', action='store_true', \n",
    "                             help='use ground truth human joint local offset.')\n",
    "parser.add_argument('--eval_oracle_dep', action='store_true', \n",
    "                             help='use ground truth depth.') '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] task\n",
      "ipykernel_launcher.py: error: the following arguments are required: task\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_opt = opt.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.eval_oracle_hmhp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpus_str\n",
    "opt.debug = max(opt.debug, 1)\n",
    "Detector = detector_factory[opt.task]\n",
    "detector = Detector(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.demo == 'webcam' or \\\n",
    "    opt.demo[opt.demo.rfind('.') + 1:].lower() in video_ext:\n",
    "    cam = cv2.VideoCapture(0 if opt.demo == 'webcam' else opt.demo)\n",
    "    detector.pause = False\n",
    "    while True:\n",
    "        _, img = cam.read()\n",
    "        cv2.imshow('input', img)\n",
    "        ret = detector.run(img)\n",
    "        time_str = ''\n",
    "        for stat in time_stats:\n",
    "          time_str = time_str + '{} {:.3f}s |'.format(stat, ret[stat])\n",
    "        print(time_str)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            return  # esc to quit\n",
    "else:\n",
    "    if os.path.isdir(opt.demo):\n",
    "      image_names = []\n",
    "      ls = os.listdir(opt.demo)\n",
    "      for file_name in sorted(ls):\n",
    "          ext = file_name[file_name.rfind('.') + 1:].lower()\n",
    "          if ext in image_ext:\n",
    "              image_names.append(os.path.join(opt.demo, file_name))\n",
    "    else:\n",
    "      image_names = [opt.demo]\n",
    "    \n",
    "    for (image_name) in image_names:\n",
    "      ret = detector.run(image_name)\n",
    "      time_str = ''\n",
    "      for stat in time_stats:\n",
    "        time_str = time_str + '{} {:.3f}s |'.format(stat, ret[stat])\n",
    "      print(time_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CenterNet] *",
   "language": "python",
   "name": "conda-env-CenterNet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
